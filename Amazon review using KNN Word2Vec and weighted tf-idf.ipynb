{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import sqlite3\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Using sqllite to fetch the data\n",
    "con=sqlite3.connect('C:\\\\Users\\\\RAJAN SHRIVASTAVA\\\\Documents\\\\AppliedAI\\\\ML Algorithms\\\\database.sqlite')\n",
    "#filtering only positive and negative reviews i.e. \n",
    "# not taking into consideration those reviews with Score=3\n",
    "filtered_data=pd.read_sql_query(\"\"\" select * from Reviews where Score !=3\"\"\",con)\n",
    "\n",
    "# Give reviews with Score>3 a positive rating, and reviews with a score<3 a negative rating.\n",
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "actualscore=filtered_data['Score']\n",
    "positiveNegative=actualscore.map(partition)\n",
    "filtered_data['Score']=positiveNegative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
       "4            Great taffy  Great taffy at a great price.  There was a wid...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.shape\n",
    "filtered_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display=pd.read_sql_query(\"\"\" select * from Reviews where HelpfulnessNumerator > HelpfulnessDenominator \"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73791</td>\n",
       "      <td>B000HDOPZG</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>78445</td>\n",
       "      <td>B000HDL1RQ</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>138277</td>\n",
       "      <td>B000HDOPYM</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138317</td>\n",
       "      <td>B000HDOPYC</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155049</td>\n",
       "      <td>B000PAQ75C</td>\n",
       "      <td>AR5J8UI46CURR</td>\n",
       "      <td>Geetha Krishnan</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1199577600</td>\n",
       "      <td>LOACKER QUADRATINI VANILLA WAFERS</td>\n",
       "      <td>DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Id   ProductId         UserId      ProfileName  HelpfulnessNumerator  \\\n",
       "0   73791  B000HDOPZG  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "1   78445  B000HDL1RQ  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "2  138277  B000HDOPYM  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "3  138317  B000HDOPYC  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "4  155049  B000PAQ75C  AR5J8UI46CURR  Geetha Krishnan                     2   \n",
       "\n",
       "   HelpfulnessDenominator  Score        Time  \\\n",
       "0                       2      5  1199577600   \n",
       "1                       2      5  1199577600   \n",
       "2                       2      5  1199577600   \n",
       "3                       2      5  1199577600   \n",
       "4                       2      5  1199577600   \n",
       "\n",
       "                             Summary  \\\n",
       "0  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "1  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "2  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "3  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "4  LOACKER QUADRATINI VANILLA WAFERS   \n",
       "\n",
       "                                                Text  \n",
       "0  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "1  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "2  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "3  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  \n",
       "4  DELICIOUS WAFERS. I FIND THAT EUROPEAN WAFERS ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display=pd.read_sql_query(\"\"\" select * from Reviews where Score !=3 and UserId='AR5J8UI46CURR' \"\"\",con)\n",
    "display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364173, 10)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sorting the data according to the productId\n",
    "sort_data=filtered_data.sort_values('ProductId',ascending=True,axis=0,inplace=False,kind='quicksort',na_position='last')\n",
    "\n",
    "final_data=sort_data.drop_duplicates(subset={'UserId','ProfileName','Time','Text'},keep='first',inplace=False)\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data=final_data[final_data.HelpfulnessNumerator<=final_data.HelpfulnessDenominator]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(364171, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I am going to take 20000 Samples as I am having Low config pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data=final_data.sample(n=20000,replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Based Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_data=final_data.sort_values('Time',ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "I'm getting crazy.<p>Is it really impossible today not to find the French VHS version of this film ?<p>Could U please tell me something about it ? Tks\n"
     ]
    }
   ],
   "source": [
    "# Find the sentences containing html tag using regular expression \n",
    "import re\n",
    "i=0\n",
    "for sent in final_data['Text'].values:\n",
    "    if (len(re.findall('<.*?>',sent))):\n",
    "        print(i)\n",
    "        print(sent)\n",
    "        break;\n",
    "    i=i+1;   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop=set(stopwords.words('english'))\n",
    "sno=nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'or', 'each', \"you've\", 'when', 'nor', 'couldn', 'didn', 'weren', 'as', 'y', 'me', 'of', 'above', \"don't\", 'needn', 'were', 'most', 'again', 'any', \"doesn't\", 'such', 'hadn', 'haven', 'do', \"mightn't\", 'did', 'what', 'which', 'once', \"aren't\", 'few', 'only', 'very', 'yours', 'no', 'down', 'below', 'then', 'should', 'on', 'that', 't', \"you're\", \"couldn't\", 'against', \"you'll\", 'after', 'hers', 'from', 'yourself', \"mustn't\", \"wasn't\", \"it's\", 'am', 'herself', 'there', 'than', 'o', 'shan', 'just', 'won', \"shouldn't\", 'aren', 'be', 'so', 'hasn', 'will', 'this', 'where', 'shouldn', 'my', 'itself', 'other', 'those', 'more', 'during', 'doesn', \"hasn't\", 'these', 'all', 'further', 'how', 'mightn', 're', \"haven't\", 'yourselves', 'is', 'into', 'an', 'was', 'he', 'some', 'own', 'wasn', 'isn', 'theirs', 'i', 'with', 'them', 'a', 'if', 'why', 'him', 'same', 'd', 'themselves', \"shan't\", \"weren't\", 'out', 'too', 'because', \"that'll\", 'while', 'ma', 'his', 'has', 'until', 'you', 'over', 'not', 'before', \"she's\", 'but', 'off', \"isn't\", 'both', 'll', 'about', 'myself', 'your', 'by', 'doing', 'we', 'they', 've', 'up', 'to', 'whom', 'now', 'm', 'mustn', 'himself', 'the', \"didn't\", 'between', 'who', 'don', \"should've\", 'her', 'have', 'does', 'our', 'can', 'wouldn', 'ours', 'its', 'in', 'ain', 'been', 'their', 'through', \"needn't\", \"hadn't\", 'she', 'are', 'having', 'being', 'and', 'at', 's', 'for', 'under', \"wouldn't\", 'ourselves', 'it', 'here', 'had', \"won't\", \"you'd\"}\n",
      "******************************************************************\n",
      "tasti\n"
     ]
    }
   ],
   "source": [
    "def cleanhtml(sentences):#function to clean html tag from the word\n",
    "    cleaner=re.compile('<.*?>')\n",
    "    cleantext=re.sub(cleaner,'',sentences)\n",
    "    return cleantext\n",
    "def cleanpunc(sentences):# function to clean the punctuation from the word\n",
    "    cleaned = re.sub(r'[?|!|\\'|\"|#]',r'',sentences)\n",
    "    cleaned = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned)\n",
    "    return  cleaned\n",
    "print(stop)\n",
    "print('******************************************************************')\n",
    "print(sno.stem('tasty'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code for implementing step-by-step the checks mentioned in the pre-processing phase\n",
    "# this code takes a while to run as it needs to run on 500k sentences.\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "all_positive_words=[]\n",
    "all_negative_words=[]\n",
    "s=''\n",
    "for sent in final_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for clean_words in cleanpunc(w).split():\n",
    "            if((clean_words.isalpha()) &(len(clean_words)>2)):\n",
    "                if (clean_words.lower() not in stop):\n",
    "                    s=(sno.stem(clean_words.lower())).encode('utf8')\n",
    "                    filtered_sentence.append(s)\n",
    "                    if(final_data['Score'].values)[i]=='Positive':\n",
    "                        all_positive_words.append(s)\n",
    "                    if(final_data['Score'].values)[i]=='Negative':\n",
    "                        all_negative_words.append(s)\n",
    "                     \n",
    "                else:\n",
    "                    continue\n",
    "            else:\n",
    "                continue\n",
    "                    \n",
    "           #print(filtered_sentence)\n",
    "    str1 = b\" \".join(filtered_sentence) #final string of cleaned words\n",
    "    #print(\"***********************************************************************\")\n",
    "    \n",
    "    final_string.append(str1)\n",
    "    i+=1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RAJAN SHRIVASTAVA\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final_data['Text'].values:\n",
    "    filtered_sentence=[]\n",
    "    sent=cleanhtml(sent)\n",
    "    for w in sent.split():\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "            if(cleaned_words.isalpha()):    \n",
    "                filtered_sentence.append(cleaned_words.lower())\n",
    "            else:\n",
    "                continue \n",
    "    list_of_sent.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_sent[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.Word2Vec(list_of_sent,min_count=5,size=50,workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words=list(w2v_model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "sent_vectors=[]\n",
    "for sent in list_of_sent:\n",
    "    sent_vec=np.zeros(50)\n",
    "    #print(sent_vec)\n",
    "    cnt_words=0;\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]\n",
    "            #print(vec)\n",
    "            sent_vec+=vec\n",
    "            cnt_words+=1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "print(len(sent_vectors))\n",
    "print(len(sent_vectors[0]))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent_vectors_array=np.array(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews=final_data['Score']\n",
    "reviews=reviews[0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now our goal is to find the best k value so that We can feed in our Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_1,X_test,Y_1,Y_test=cross_validation.train_test_split(sent_vectors,reviews,test_size=0.3,random_state=0)\n",
    "#X_train,X_cv,Y_train,Y_cv=cross_validation.train_test_split(X_1,Y_1,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tscv=TimeSeriesSplit(n_splits=3)\n",
    "#reviews=reviews.values.reshape(-1,1)\n",
    "for train_index, test_index in tscv.split(sent_vectors_array):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train, X_test = sent_vectors_array[train_index], sent_vectors_array[test_index]\n",
    "    Y_train, Y_test = reviews[train_index], reviews[test_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train=Y_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating odd list of K for KNN\n",
    "myList = list(range(0,50))\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k \n",
    "plt.plot(neighbors, MSE)\n",
    "\n",
    "for xy in zip(neighbors, np.round(MSE,3)):\n",
    "    plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n",
    "\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()\n",
    "\n",
    "print(\"the misclassification error for each k value is : \", np.round(MSE,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation ---Here We have got a very interesting curve, The MSE is high on the Low value of the K as the K value increases ,our MSE starts decreasing and becomes staturated at k =21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Knn_optimal=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\n",
    "#Fitting the Model\n",
    "Knn_optimal.fit(X_train,Y_train)\n",
    "#Predicting the Model\n",
    "predict=Knn_optimal.predict(X_test)\n",
    "#Evaluate accuracy\n",
    "acc=accuracy_score(Y_test,predict)*float(100)\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Knn_optimal=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\n",
    "#Fitting the Model\n",
    "Knn_optimal.fit(X_train,Y_train)\n",
    "#Predicting the Model\n",
    "predict=Knn_optimal.predict(X_test)\n",
    "#Evaluate accuracy\n",
    "acc=accuracy_score(Y_test,predict)*float(100)\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm=metrics.confusion_matrix(Y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds=roc_curve(Y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roc_auc=auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr,tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'y--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TN=cm[0][0]\n",
    "FN=cm[0][1]\n",
    "FP=cm[1][0]\n",
    "TP=cm[1][1]\n",
    "P=FN+TP\n",
    "N=TN+FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_positive_rate=TP/P\n",
    "true_negative_rate=TN/N\n",
    "false_positive_rate=FP/N\n",
    "false_negative_rate=FN/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('TPR',true_positive_rate,'TNR =',true_negative_rate,'FPR =',false_positive_rate,'FNR =',false_negative_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation----Our TP=4991 from confusion matrix , There are the reviews which are positive and our Model is also predicted positive\n",
    "#TN=106 These are the negative review and Our Model has also predicted negative\n",
    "##FN=51 These are the positive reviews but our model predicted negative\n",
    "FP=863 These are negative reviews but predicted positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision:\n",
    "    \n",
    "By looking into classification Report , We have a nice value of the Precision=0.85 , It tells us that Total numner of the predicted positive reviews in which how many are actual positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall:\n",
    "Our Recall =0.99 , It tells us that total acutal positive reviews out of which how many positive reviews are predicted by the model      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF weighted Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We have taken only uni-grams because of memory issues\n",
    "tf_idf_vec=TfidfVectorizer()\n",
    "final_tf_idf=tf_idf_vec.fit_transform(final_data['Text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_idf_feat=tf_idf_vec.get_feature_names()\n",
    "tf_idf_sent_vectors=[];\n",
    "#Storing each review/sentence in the list\n",
    "row=0;\n",
    "for sent in list_of_sent:#for each review/sentence\n",
    "    sent_vec=np.zeros(50)\n",
    "    weight_sum=0\n",
    "    for word in sent:\n",
    "        try:\n",
    "            vec=w2v_model.wv[word]\n",
    "            tfidf=final_tf_idf[row,tf_idf_feat.index(word)]\n",
    "            sent_vec +=(vec*tfidf)\n",
    "            weight_sum +=tfidf\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= weight_sum\n",
    "    tf_idf_sent_vectors.append(sent_vec)\n",
    "    #print(tf_idf_sent_vectors)\n",
    "    row += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To fit in our algorithm convert the list into array\n",
    "tf_idf_array=np.array(tf_idf_sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_1,X_test,Y_1,Y_test=cross_validation.train_test_split(tf_idf_sent_vectors,reviews,test_size=0.3,random_state=0)\n",
    "#X_train,X_cv,Y_train,Y_cv=cross_validation.train_test_split(X_1,Y_1,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tscv=TimeSeriesSplit(n_splits=3)\n",
    "reviews=reviews.values.reshape(-1,1)\n",
    "for train_index, test_index in tscv.split(tf_idf_sent_vectors):\n",
    "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
    "    X_train, X_test = tf_idf_sent_vectors[train_index], tf_idf_sent_vectors[test_index]\n",
    "    Y_train, Y_test = reviews[train_index], reviews[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train=Y_train.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating odd list of K for KNN\n",
    "myList = list(range(0,50))\n",
    "neighbors = list(filter(lambda x: x % 2 != 0, myList))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "\n",
    "# perform 10-fold cross validation\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train, Y_train, cv=10, scoring='accuracy')\n",
    "    cv_scores.append(scores.mean())\n",
    "\n",
    "# changing to misclassification error\n",
    "MSE = [1 - x for x in cv_scores]\n",
    "\n",
    "# determining best k\n",
    "optimal_k = neighbors[MSE.index(min(MSE))]\n",
    "print('\\nThe optimal number of neighbors is %d.' % optimal_k)\n",
    "\n",
    "# plot misclassification error vs k \n",
    "plt.plot(neighbors, MSE)\n",
    "\n",
    "for xy in zip(neighbors, np.round(MSE,3)):\n",
    "    plt.annotate('(%s, %s)' % xy, xy=xy, textcoords='data')\n",
    "\n",
    "plt.xlabel('Number of Neighbors K')\n",
    "plt.ylabel('Misclassification Error')\n",
    "plt.show()\n",
    "\n",
    "print(\"the misclassification error for each k value is : \", np.round(MSE,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Knn_optimal=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='brute')\n",
    "#Fitting the Model\n",
    "Knn_optimal.fit(X_train,Y_train)\n",
    "#Predicting the Model\n",
    "predict=Knn_optimal.predict(X_test)\n",
    "#Evaluate accuracy\n",
    "acc=accuracy_score(Y_test,predict)*float(100)\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Knn_optimal=KNeighborsClassifier(n_neighbors=optimal_k,algorithm='kd_tree')\n",
    "#Fitting the Model\n",
    "Knn_optimal.fit(X_train,Y_train)\n",
    "#Predicting the Model\n",
    "predict=Knn_optimal.predict(X_test)\n",
    "#Evaluate accuracy\n",
    "acc=accuracy_score(Y_test,predict)*float(100)\n",
    "print('\\nThe accuracy of the knn classifier for k = %d is %f%%' % (optimal_k, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "cm=metrics.confusion_matrix(Y_test,predict)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(Y_test,predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr,tpr,thresholds=roc_curve(Y_test,predict)\n",
    "roc_auc=auc(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr,tpr, 'b',label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'y--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TN=cm[0][0]\n",
    "FN=cm[0][1]\n",
    "FP=cm[1][0]\n",
    "TP=cm[1][1]\n",
    "P=FN+TP\n",
    "N=TN+FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true_positive_rate=TP/P\n",
    "true_negative_rate=TN/N\n",
    "false_positive_rate=FP/N\n",
    "false_negative_rate=FN/P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('TPR',true_positive_rate,'TNR =',true_negative_rate,'FPR =',false_positive_rate,'FNR =',false_negative_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation----Our TP=5042 from confusion matrix , There are the reviews which are positive and our Model is also predicted positive\n",
    "#TN=110 These are the negative review and Our Model has also predicted negative\n",
    "##FN=51 These are the positive reviews but our model predicted negative\n",
    "FP=797 These are negative reviews but predicted positive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision:\n",
    "    \n",
    "By looking into classification Report , We have a nice value of the Precision=0.86 , It tells us that Total numner of the predicted positive reviews in which how many are actual positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recall:\n",
    "Our Recall =0.99 , It tells us that total acutal positive reviews out of which how many positive reviews are predicted by the model      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
