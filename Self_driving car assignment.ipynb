{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from scipy import pi\n",
    "from itertools import islice\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import skimage.transform as sk\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import keras.backend as k\n",
    "k.clear_session()\n",
    "import tensorflow as tf\n",
    "import scipy.misc\n",
    "import cv2\n",
    "from subprocess import call\n",
    "import math\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Changes\n",
    "1. 70-30%\n",
    "2. Adam- 1e-3\n",
    "3. dropout=0.5\n",
    "4. Final Layer: Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing data.txt\n"
     ]
    }
   ],
   "source": [
    "DATA_FOLDER = './driving_dataset/' # change this to your folder\n",
    "TRAIN_FILE = os.path.join(DATA_FOLDER, 'data.txt')\n",
    "\n",
    "\n",
    "split =0.7\n",
    "X = []\n",
    "y = []\n",
    "LIMIT=None\n",
    "with open(TRAIN_FILE) as fp:\n",
    "    \n",
    "    for line in islice(fp,LIMIT):\n",
    "        \n",
    "        path, angle = line.strip().split()\n",
    "        full_path = os.path.join(DATA_FOLDER, path)\n",
    "        X.append(full_path)\n",
    "        \n",
    "        # converting angle from degrees to radians\n",
    "        y.append(float(angle) * pi / 180 )\n",
    "\n",
    "\n",
    "y = np.array(y)\n",
    "print(\"Completed processing data.txt\")\n",
    "\n",
    "split_index = int(len(y)*0.7)\n",
    "\n",
    "train_y = y[:split_index]\n",
    "test_y = y[split_index:]          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_y, bins=50, normed=1, color='green', histtype ='step');\n",
    "plt.hist(test_y, bins=50, normed=1, color='red', histtype ='step');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test_MSE(MEAN):0.241561\n",
      "Test_MSE(ZERO):0.241107\n"
     ]
    }
   ],
   "source": [
    "train_mean_y = np.mean(train_y)\n",
    "\n",
    "print('Test_MSE(MEAN):%f' % np.mean(np.square(test_y-train_mean_y)) )\n",
    "\n",
    "print('Test_MSE(ZERO):%f' % np.mean(np.square(test_y-0.0)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=[]\n",
    "ys=[]\n",
    "\n",
    "train_batch_pointer=0\n",
    "val_batch_pointer=0\n",
    "\n",
    "with open('driving_dataset/data.txt') as f:\n",
    "    for line in f:\n",
    "        xs.append(\"driving_dataset/\" + line.split()[0])\n",
    "        ys.append(float(line.split()[1])*scipy.pi / 180)\n",
    "num_images=len(xs)\n",
    "train_xs=xs[:int(len(xs)*0.7)]\n",
    "train_ys=ys[:int(len(xs)*0.7)]\n",
    "\n",
    "val_xs=xs[-int(len(xs)*0.3):]\n",
    "val_ys=ys[-int(len(ys)*0.3):]\n",
    "\n",
    "num_train_images=len(train_xs)\n",
    "num_val_images=len(val_xs)\n",
    "\n",
    "def LoadTrainBatch(batch_size):\n",
    "    global train_batch_pointer\n",
    "    x_out=[]\n",
    "    y_out=[]\n",
    "    for i in range(0,batch_size):\n",
    "        x_out.append(sk.resize(imageio.imread(train_xs[(train_batch_pointer +i)%num_train_images])[-150:],[66,200])/255.0)\n",
    "        y_out.append([train_ys[(train_batch_pointer +i)%num_train_images]])\n",
    "    train_batch_pointer +=batch_size\n",
    "    return x_out,y_out\n",
    "\n",
    "def LoadValBatch(batch_size):\n",
    "    global val_batch_pointer\n",
    "    x_out = []\n",
    "    y_out = []\n",
    "    for i in range(0, batch_size):\n",
    "        x_out.append(sk.resize(imageio.imread(val_xs[(val_batch_pointer + i) % num_val_images])[-150:], [66, 200]) / 255.0)\n",
    "        y_out.append([val_ys[(val_batch_pointer + i) % num_val_images]])\n",
    "    val_batch_pointer += batch_size\n",
    "    return x_out, y_out    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W, stride):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='VALID')\n",
    "\n",
    "x = tf.placeholder(dtype=tf.float32, shape=[None, 66, 200, 3])\n",
    "y_ = tf.placeholder(dtype=tf.float32, shape=[None, 1])\n",
    "\n",
    "x_image = x\n",
    "\n",
    "#first convolutional layer\n",
    "W_conv1 = weight_variable([5, 5, 3, 24])\n",
    "b_conv1 = bias_variable([24])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1, 2) + b_conv1)\n",
    "\n",
    "#second convolutional layer\n",
    "W_conv2 = weight_variable([5, 5, 24, 36])\n",
    "b_conv2 = bias_variable([36])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, W_conv2, 2) + b_conv2)\n",
    "\n",
    "#third convolutional layer\n",
    "W_conv3 = weight_variable([5, 5, 36, 48])\n",
    "b_conv3 = bias_variable([48])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, W_conv3, 2) + b_conv3)\n",
    "\n",
    "#fourth convolutional layer\n",
    "W_conv4 = weight_variable([3, 3, 48, 64])\n",
    "b_conv4 = bias_variable([64])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_conv3, W_conv4, 1) + b_conv4)\n",
    "\n",
    "#fifth convolutional layer\n",
    "W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "b_conv5 = bias_variable([64])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_conv4, W_conv5, 1) + b_conv5)\n",
    "\n",
    "#FCL 1\n",
    "W_fc1 = weight_variable([1152, 1164])\n",
    "b_fc1 = bias_variable([1164])\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 1152])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=0.5)\n",
    "\n",
    "#FCL 2\n",
    "W_fc2 = weight_variable([1164, 100])\n",
    "b_fc2 = bias_variable([100])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob=0.5)\n",
    "\n",
    "#FCL 3\n",
    "W_fc3 = weight_variable([100, 50])\n",
    "b_fc3 = bias_variable([50])\n",
    "\n",
    "h_fc3 = tf.nn.relu(tf.matmul(h_fc2_drop, W_fc3) + b_fc3)\n",
    "\n",
    "h_fc3_drop = tf.nn.dropout(h_fc3, keep_prob=0.5)\n",
    "\n",
    "#FCL 3\n",
    "W_fc4 = weight_variable([50, 10])\n",
    "b_fc4 = bias_variable([10])\n",
    "\n",
    "h_fc4 = tf.matmul(h_fc3_drop, W_fc4) + b_fc4\n",
    "\n",
    "h_fc4_drop = tf.nn.dropout(h_fc4, keep_prob=0.5)\n",
    "\n",
    "#Output\n",
    "W_fc5 = weight_variable([10, 1])\n",
    "b_fc5 = bias_variable([1])\n",
    "\n",
    "y = tf.multiply(tf.keras.activations.linear(tf.matmul(h_fc4_drop, W_fc5) + b_fc5), 2) #scale the atan output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.core.protobuf import saver_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Step: 0, Loss: 19.0207\n",
      "Epoch: 0, Step: 10, Loss: 13.0815\n",
      "Epoch: 0, Step: 20, Loss: 12.4238\n",
      "Epoch: 0, Step: 30, Loss: 12.1631\n",
      "Epoch: 0, Step: 40, Loss: 12.1418\n",
      "Epoch: 0, Step: 50, Loss: 11.6374\n",
      "Epoch: 0, Step: 60, Loss: 11.6222\n",
      "Epoch: 0, Step: 70, Loss: 11.6984\n",
      "Epoch: 0, Step: 80, Loss: 11.4144\n",
      "Epoch: 0, Step: 90, Loss: 10.9907\n",
      "Epoch: 0, Step: 100, Loss: 10.8496\n",
      "Epoch: 0, Step: 110, Loss: 10.6736\n",
      "Epoch: 0, Step: 120, Loss: 10.5324\n",
      "Epoch: 0, Step: 130, Loss: 10.6736\n",
      "Epoch: 0, Step: 140, Loss: 11.0713\n",
      "Epoch: 0, Step: 150, Loss: 10.4043\n",
      "Epoch: 0, Step: 160, Loss: 10.8902\n",
      "Epoch: 0, Step: 170, Loss: 10.0848\n",
      "Epoch: 0, Step: 180, Loss: 10.4332\n",
      "Epoch: 0, Step: 190, Loss: 9.97241\n",
      "Epoch: 0, Step: 200, Loss: 9.48776\n",
      "Epoch: 0, Step: 210, Loss: 9.35494\n",
      "Epoch: 0, Step: 220, Loss: 9.23972\n",
      "Epoch: 0, Step: 230, Loss: 9.22471\n",
      "Epoch: 0, Step: 240, Loss: 10.076\n",
      "Epoch: 0, Step: 250, Loss: 9.08156\n",
      "Epoch: 0, Step: 260, Loss: 11.0373\n",
      "Epoch: 0, Step: 270, Loss: 9.29244\n",
      "Epoch: 0, Step: 280, Loss: 9.73167\n",
      "Epoch: 0, Step: 290, Loss: 8.5948\n",
      "Epoch: 0, Step: 300, Loss: 8.37257\n",
      "Epoch: 0, Step: 310, Loss: 8.45373\n",
      "Epoch: 0, Step: 320, Loss: 9.43959\n",
      "Epoch: 0, Step: 330, Loss: 8.63214\n",
      "Epoch: 0, Step: 340, Loss: 8.18475\n",
      "Epoch: 0, Step: 350, Loss: 7.93139\n",
      "Epoch: 0, Step: 360, Loss: 7.85088\n",
      "Epoch: 0, Step: 370, Loss: 7.95876\n"
     ]
    }
   ],
   "source": [
    "LOGDIR = './My_save_4'\n",
    "sees=tf.InteractiveSession()\n",
    "L2NormConst = 0.001\n",
    "train_vars=tf.trainable_variables()\n",
    "loss = tf.reduce_mean(tf.square(tf.subtract(y_,y))) + tf.add_n([tf.nn.l2_loss(v) for v in train_vars]) * L2NormConst\n",
    "train_step=tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
    "sees.run(tf.global_variables_initializer())\n",
    "\n",
    "tf.summary.scalar('loss',loss)\n",
    "merged_summary_op=tf.summary.merge_all()\n",
    "\n",
    "saver=tf.train.Saver(write_version=tf.train.SaverDef.V2)\n",
    "\n",
    "logs_path = './my_logs_4'\n",
    "summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "\n",
    "epochs = 30\n",
    "batch_size = 100\n",
    "for epoch in range(epochs):\n",
    "    for i in range(int(num_images/batch_size)):\n",
    "        xs,ys=LoadTrainBatch(batch_size)\n",
    "        train_step.run(feed_dict={x:xs,y_:ys,keep_prob:0.6},session=sees)\n",
    "        if i % 10 == 0:\n",
    "            xs,ys=LoadValBatch(batch_size)\n",
    "            loss_value=loss.eval(feed_dict={x:xs,y_:ys,keep_prob:1.0},session=sees)\n",
    "            print(\"Epoch: %d, Step: %d, Loss: %g\" % (epoch, epoch * batch_size + i, loss_value))\n",
    "            \n",
    "        summary=merged_summary_op.eval(feed_dict={x:xs,y_:ys,keep_prob:1.0},session=sees)\n",
    "        summary_writer.add_summary(summary, epoch * num_images/batch_size + i)\n",
    "        \n",
    "        if i % batch_size==0:\n",
    "            if not os.path.exists(LOGDIR):\n",
    "                os.makedirs(LOGDIR)\n",
    "            checkpoint_path=os.path.join(LOGDIR,'my_model_4.ckpt')\n",
    "            filename = saver.save(sees, checkpoint_path)\n",
    "        #print(\"Model saved in file: %s\" % filename)                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install opencv-python\n",
    "sess = tf.InteractiveSession()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, \"save/model.ckpt\")\n",
    "\n",
    "img = cv2.imread('steering_wheel_image.jpg',0)\n",
    "rows,cols = img.shape\n",
    "\n",
    "smoothed_angle = 0\n",
    "\n",
    "\n",
    "#read data.txt\n",
    "xs = []\n",
    "ys = []\n",
    "with open(\"driving_dataset/data.txt\") as f:\n",
    "    for line in f:\n",
    "        xs.append(\"driving_dataset/\" + line.split()[0])\n",
    "        #the paper by Nvidia uses the inverse of the turning radius,\n",
    "        #but steering wheel angle is proportional to the inverse of turning radius\n",
    "        #so the steering wheel angle in radians is used as the output\n",
    "        ys.append(float(line.split()[1]) * scipy.pi / 180)\n",
    "\n",
    "#get number of images\n",
    "num_images = len(xs)\n",
    "\n",
    "\n",
    "i = math.ceil(num_images*0.8)\n",
    "print(\"Starting frameofvideo:\" +str(i))\n",
    "\n",
    "while(cv2.waitKey(10) != ord('q')):\n",
    "    full_image = scipy.misc.imread(\"driving_dataset/\" + str(i) + \".jpg\", mode=\"RGB\")\n",
    "    image = scipy.misc.imresize(full_image[-150:], [66, 200]) / 255.0\n",
    "    degrees = model.y.eval(feed_dict={model.x: [image], model.keep_prob: 1.0})[0][0] * 180.0 / scipy.pi\n",
    "    #call(\"clear\")\n",
    "    #print(\"Predicted Steering angle: \" + str(degrees))\n",
    "    print(\"Steering angle: \" + str(degrees) + \" (pred)\\t\" + str(ys[i]*180/scipy.pi) + \" (actual)\")\n",
    "    cv2.imshow(\"frame\", cv2.cvtColor(full_image, cv2.COLOR_RGB2BGR))\n",
    "    #make smooth angle transitions by turning the steering wheel based on the difference of the current angle\n",
    "    #and the predicted angle\n",
    "    smoothed_angle += 0.2 * pow(abs((degrees - smoothed_angle)), 2.0 / 3.0) * (degrees - smoothed_angle) / abs(degrees - smoothed_angle)\n",
    "    M = cv2.getRotationMatrix2D((cols/2,rows/2),-smoothed_angle,1)\n",
    "    dst = cv2.warpAffine(img,M,(cols,rows))\n",
    "    cv2.imshow(\"steering wheel\", dst)\n",
    "    i += 1\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
