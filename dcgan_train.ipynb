{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "moXVgCTuWJye",
    "outputId": "7dae6416-8513-48cc-dbd8-beb7c1a9800b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Conv2DTranspose, Reshape\n",
    "from keras.layers import Flatten, BatchNormalization, Dense, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdD6hkz_M7XA"
   },
   "outputs": [],
   "source": [
    "def load_dataset(dataset_path,batch_size,image_shape):\n",
    "    dataset_generator=ImageDataGenerator()\n",
    "    dataset_generator=dataset_generator.flow_from_directory(dataset_path,target_size=(image_shape[0],image_shape[1]),batch_size=batch_size,class_mode=None)\n",
    "    return dataset_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o_6QMewK8LU0"
   },
   "outputs": [],
   "source": [
    "def discriminator_nn(image_shape):\n",
    "    discriminator=Sequential()\n",
    "    discriminator.add(Conv2D(filters=64,kernel_size=(5,5),kernel_initializer='glorot_uniform',padding='same',strides=(2,2),input_shape=(image_shape)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "  \n",
    "    discriminator.add(Conv2D(filters=128,kernel_size=(5,5),padding='same',strides=(2,2),kernel_initializer='glorot_uniform',data_format='channels_last'))\n",
    "    discriminator.add(BatchNormalization(momentum=0.5))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(2, 2), padding='same',data_format='channels_last', kernel_initializer='glorot_uniform'))\n",
    "    discriminator.add(BatchNormalization(momentum=0.5))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "\n",
    "    discriminator.add(Conv2D(filters=512, kernel_size=(5, 5),strides=(2, 2), padding='same',data_format='channels_last',kernel_initializer='glorot_uniform'))\n",
    "    discriminator.add(BatchNormalization(momentum=0.5))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Flatten())\n",
    "    discriminator.add(Dense(1))\n",
    "    discriminator.add(Activation('sigmoid'))\n",
    "    optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "    discriminator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=None)\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Ins2UIVAXuJ"
   },
   "outputs": [],
   "source": [
    "def generator_nn():\n",
    "    generator=Sequential()\n",
    "    generator.add(Dense(units=4 * 4 * 512,kernel_initializer='glorot_uniform',input_shape=(1, 1, 100)))\n",
    "    generator.add(Reshape(target_shape=(4, 4, 512)))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "    generator.add(Conv2DTranspose(filters=256,kernel_initializer='glorot_uniform',kernel_size=(5,5),padding='same',strides=(2,2),data_format='channels_last'))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "    generator.add(Conv2DTranspose(filters=128, kernel_size=(5, 5),strides=(2, 2), padding='same',data_format='channels_last',kernel_initializer='glorot_uniform'))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Conv2DTranspose(filters=64, kernel_size=(5, 5),strides=(2, 2), padding='same',data_format='channels_last',kernel_initializer='glorot_uniform'))\n",
    "    generator.add(BatchNormalization(momentum=0.5))\n",
    "    generator.add(Activation('relu'))\n",
    "    generator.add(Conv2DTranspose(filters=3, kernel_size=(5, 5),strides=(2, 2), padding='same',data_format='channels_last',kernel_initializer='glorot_uniform'))\n",
    "    generator.add(Activation('tanh'))\n",
    "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
    "    generator.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=None)\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yu2kDhjqEr7T"
   },
   "outputs": [],
   "source": [
    "def save_the_generated_images(generated_images, epoch, batch_number):\n",
    "    plt.figure(figsize=(8,8),num=2)\n",
    "    gs1 = gridspec.GridSpec(8, 8)\n",
    "    gs1.update(wspace=0, hspace=0)\n",
    "    for i in range(64):\n",
    "        ax1=plt.subplot(gs1[i])\n",
    "        ax1.set_aspect('equal')\n",
    "        image = generated_images[i, :, :, :]\n",
    "        image += 1\n",
    "        image *= 127.5\n",
    "        fig = plt.imshow(image.astype(np.uint8))\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    save_name='generated images/' + str(epoch + 1)+ '_batch' + str(batch_number + 1) + '.png'\n",
    "    plt.savefig(save_name, bbox_inches='tight', pad_inches=0)\n",
    "    plt.pause(0.0000000001)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the DCGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L_f6e3GJhrOm"
   },
   "outputs": [],
   "source": [
    "def train_dcgan(batch_size,epochs,image_shape,dataset_path):\n",
    "    \n",
    "    generator=generator_nn()\n",
    "    discriminator=discriminator_nn(image_shape)\n",
    "    gan = Sequential()\n",
    "    discriminator.trainable = False\n",
    "    gan.add(generator)\n",
    "    gan.add(discriminator)\n",
    "    optimizer = Adam(lr=0.00015, beta_1=0.5)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=None)\n",
    "    dataset_generator=load_dataset(dataset_path, batch_size, image_shape)\n",
    "    number_of_batches = int(11988 / batch_size)\n",
    "    adversarial_loss = np.empty(shape=1)\n",
    "    discriminator_loss = np.empty(shape=1)\n",
    "    batches = np.empty(shape=1)\n",
    "    plt.ion()\n",
    "  \n",
    "    current_batch = 0\n",
    "  \n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch \" + str(epoch+1) + \"/\" + str(epochs) + \" :\")\n",
    "    \n",
    "        for batch_number in range(number_of_batches):\n",
    "            start_time = time.time()\n",
    "            real_images = dataset_generator.next()\n",
    "            real_images /= 127.5\n",
    "            real_images -= 1\n",
    "            current_batch_size = real_images.shape[0]\n",
    "            noise=np.random.normal(0,1,size=(current_batch_size,) +(1,1,100))\n",
    "            generated_images = generator.predict(noise)\n",
    "            real_y = (np.ones(current_batch_size) - np.random.random_sample(current_batch_size) * 0.2)\n",
    "            fake_y = np.random.random_sample(current_batch_size) * 0.2\n",
    "            discriminator.trainable = True\n",
    "            d_loss = discriminator.train_on_batch(real_images, real_y)\n",
    "            d_loss += discriminator.train_on_batch(generated_images, fake_y)\n",
    "            discriminator_loss = np.append(discriminator_loss, d_loss)\n",
    "      \n",
    "            discriminator.trainable = False\n",
    "      \n",
    "            noise = np.random.normal(0, 1,size=(current_batch_size * 2,) +(1, 1, 100))\n",
    "            fake_y = (np.ones(current_batch_size * 2) - np.random.random_sample(current_batch_size * 2) * 0.2)\n",
    "      \n",
    "            g_loss = gan.train_on_batch(noise, fake_y)\n",
    "            adversarial_loss = np.append(adversarial_loss, g_loss)\n",
    "            batches = np.append(batches, current_batch)\n",
    "            if ((batch_number + 1) % 50 == 0 and current_batch_size == batch_size):\n",
    "                save_the_generated_images(generated_images, epoch, batch_number)\n",
    "                time_elapsed = time.time() - start_time\n",
    "        \n",
    "                print(\"Batch \" + str(batch_number + 1) + \"/\" + str(number_of_batches) +\n",
    "                      \" generator loss | discriminator loss : \" +str(g_loss) + \" | \" + str(d_loss) + ' - batch took ' + str(time_elapsed) + ' s.')\n",
    "        \n",
    "                current_batch += 1\n",
    "        \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                discriminator.trainable = True\n",
    "                generator.save('models/generator_epoch/' + str(epoch) + '.hdf5')\n",
    "                discriminator.save('models/discriminator_epoch/' +str(epoch) + '.hdf5')\n",
    "          \n",
    "                plt.figure(1)\n",
    "                plt.plot(batches, adversarial_loss, color='green',label='Generator Loss')\n",
    "                plt.plot(batches, discriminator_loss, color='blue',label='Discriminator Loss')\n",
    "                plt.title(\"DCGAN Train\")\n",
    "                plt.xlabel(\"Batch Iteration\")\n",
    "                plt.ylabel(\"Loss\")\n",
    "                if epoch == 0:\n",
    "                    plt.legend()\n",
    "                    plt.pause(0.0000000001)\n",
    "                    plt.show()\n",
    "                    plt.savefig('trainingLossPlot.png')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 26710
    },
    "colab_type": "code",
    "id": "Fup9ZHGP7NfY",
    "outputId": "f717f1e6-deea-4f86-a44b-f4f4977de55f"
   },
   "source": [
    "dataset_path='images/'\n",
    "batch_size = 64\n",
    "image_shape = (64, 64, 3)\n",
    "epochs = 190\n",
    "train_dcgan(batch_size, epochs,image_shape, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwQr3guk8jH1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13liMQn6vQOR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
